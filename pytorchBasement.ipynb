{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp/ipykernel_12868/2926312356.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[1;32m----> 1\u001B[1;33m \u001B[1;32mimport\u001B[0m \u001B[0mtorch\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      2\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mnumpy\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0mnp\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      3\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mautograd\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mVariable\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0mvar\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mModuleNotFoundError\u001B[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch.autograd import Variable as var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.04201571 -1.03146311 -0.99324209  2.23482718  0.48731153  0.04141532\n",
      "  -1.87805052  0.23430846  0.89011077 -2.40878108  0.76822011 -0.80727184\n",
      "  -0.23139028  1.09034945  0.64625316 -0.73427032  1.71698982 -0.47911683\n",
      "  -1.04346074  0.44238988]\n",
      " [-0.54721813  1.44165435 -0.03969733 -2.75559713  0.49706977  0.86670278\n",
      "  -0.32585497  1.12179698 -1.17537943 -0.84064684 -0.50501519  1.17139473\n",
      "   0.37041284 -1.20415734  1.15911531  0.32632547  1.10950109  1.09967516\n",
      "   0.12099883 -1.07209097]\n",
      " [ 1.92242757  0.91221825  1.46721681  0.58711888 -1.08791583 -0.10710813\n",
      "  -2.23264201  0.49413365 -0.64166269  0.27184266 -0.74354951 -0.22969058\n",
      "  -1.42595282  1.14184497  0.32993744 -1.74405513  0.66325176  0.62165473\n",
      "   2.75657771  1.88827242]\n",
      " [ 0.78623627  1.36798176  0.63751017 -0.13390405 -0.26573913  2.27166243\n",
      "  -1.22809219  1.1004839  -0.42809814 -0.8377606  -0.95980261 -0.40412786\n",
      "  -0.22005714  1.99172938  1.19272635 -0.45822569  0.81218832 -0.74867033\n",
      "   0.79247226 -0.05058666]\n",
      " [-0.47727546 -2.38124478  0.96219759 -0.61945695  0.06275528  0.04984297\n",
      "   0.04124314 -1.44747979  0.70457429 -0.20346952 -0.56703614 -0.57907437\n",
      "   1.34388531  0.28976773 -0.26311703 -0.19356135  1.14737027  0.5274203\n",
      "  -0.13148319 -0.48126731]] tensor([[-0.0420, -1.0315, -0.9932,  2.2348,  0.4873,  0.0414, -1.8781,  0.2343,\n",
      "          0.8901, -2.4088,  0.7682, -0.8073, -0.2314,  1.0903,  0.6463, -0.7343,\n",
      "          1.7170, -0.4791, -1.0435,  0.4424],\n",
      "        [-0.5472,  1.4417, -0.0397, -2.7556,  0.4971,  0.8667, -0.3259,  1.1218,\n",
      "         -1.1754, -0.8406, -0.5050,  1.1714,  0.3704, -1.2042,  1.1591,  0.3263,\n",
      "          1.1095,  1.0997,  0.1210, -1.0721],\n",
      "        [ 1.9224,  0.9122,  1.4672,  0.5871, -1.0879, -0.1071, -2.2326,  0.4941,\n",
      "         -0.6417,  0.2718, -0.7435, -0.2297, -1.4260,  1.1418,  0.3299, -1.7441,\n",
      "          0.6633,  0.6217,  2.7566,  1.8883],\n",
      "        [ 0.7862,  1.3680,  0.6375, -0.1339, -0.2657,  2.2717, -1.2281,  1.1005,\n",
      "         -0.4281, -0.8378, -0.9598, -0.4041, -0.2201,  1.9917,  1.1927, -0.4582,\n",
      "          0.8122, -0.7487,  0.7925, -0.0506],\n",
      "        [-0.4773, -2.3812,  0.9622, -0.6195,  0.0628,  0.0498,  0.0412, -1.4475,\n",
      "          0.7046, -0.2035, -0.5670, -0.5791,  1.3439,  0.2898, -0.2631, -0.1936,\n",
      "          1.1474,  0.5274, -0.1315, -0.4813]], dtype=torch.float64) tensor([[-0.0420, -1.0315, -0.9932,  2.2348,  0.4873,  0.0414, -1.8781,  0.2343,\n",
      "          0.8901, -2.4088,  0.7682, -0.8073, -0.2314,  1.0903,  0.6463, -0.7343,\n",
      "          1.7170, -0.4791, -1.0435,  0.4424],\n",
      "        [-0.5472,  1.4417, -0.0397, -2.7556,  0.4971,  0.8667, -0.3259,  1.1218,\n",
      "         -1.1754, -0.8406, -0.5050,  1.1714,  0.3704, -1.2042,  1.1591,  0.3263,\n",
      "          1.1095,  1.0997,  0.1210, -1.0721],\n",
      "        [ 1.9224,  0.9122,  1.4672,  0.5871, -1.0879, -0.1071, -2.2326,  0.4941,\n",
      "         -0.6417,  0.2718, -0.7435, -0.2297, -1.4260,  1.1418,  0.3299, -1.7441,\n",
      "          0.6633,  0.6217,  2.7566,  1.8883],\n",
      "        [ 0.7862,  1.3680,  0.6375, -0.1339, -0.2657,  2.2717, -1.2281,  1.1005,\n",
      "         -0.4281, -0.8378, -0.9598, -0.4041, -0.2201,  1.9917,  1.1927, -0.4582,\n",
      "          0.8122, -0.7487,  0.7925, -0.0506],\n",
      "        [-0.4773, -2.3812,  0.9622, -0.6195,  0.0628,  0.0498,  0.0412, -1.4475,\n",
      "          0.7046, -0.2035, -0.5670, -0.5791,  1.3439,  0.2898, -0.2631, -0.1936,\n",
      "          1.1474,  0.5274, -0.1315, -0.4813]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "# tenor and numpy\n",
    "## numpy 转化为torch的tensor的两种办法\n",
    "numpy_tensor = np.random.randn(5, 20)\n",
    "pytorch_tensor1 = torch.tensor(numpy_tensor)\n",
    "pytorch_tensor2 = torch.from_numpy(numpy_tensor)\n",
    "print(numpy_tensor, pytorch_tensor1, pytorch_tensor2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.04201571 -1.03146311 -0.99324209  2.23482718  0.48731153  0.04141532\n",
      "  -1.87805052  0.23430846  0.89011077 -2.40878108  0.76822011 -0.80727184\n",
      "  -0.23139028  1.09034945  0.64625316 -0.73427032  1.71698982 -0.47911683\n",
      "  -1.04346074  0.44238988]\n",
      " [-0.54721813  1.44165435 -0.03969733 -2.75559713  0.49706977  0.86670278\n",
      "  -0.32585497  1.12179698 -1.17537943 -0.84064684 -0.50501519  1.17139473\n",
      "   0.37041284 -1.20415734  1.15911531  0.32632547  1.10950109  1.09967516\n",
      "   0.12099883 -1.07209097]\n",
      " [ 1.92242757  0.91221825  1.46721681  0.58711888 -1.08791583 -0.10710813\n",
      "  -2.23264201  0.49413365 -0.64166269  0.27184266 -0.74354951 -0.22969058\n",
      "  -1.42595282  1.14184497  0.32993744 -1.74405513  0.66325176  0.62165473\n",
      "   2.75657771  1.88827242]\n",
      " [ 0.78623627  1.36798176  0.63751017 -0.13390405 -0.26573913  2.27166243\n",
      "  -1.22809219  1.1004839  -0.42809814 -0.8377606  -0.95980261 -0.40412786\n",
      "  -0.22005714  1.99172938  1.19272635 -0.45822569  0.81218832 -0.74867033\n",
      "   0.79247226 -0.05058666]\n",
      " [-0.47727546 -2.38124478  0.96219759 -0.61945695  0.06275528  0.04984297\n",
      "   0.04124314 -1.44747979  0.70457429 -0.20346952 -0.56703614 -0.57907437\n",
      "   1.34388531  0.28976773 -0.26311703 -0.19356135  1.14737027  0.5274203\n",
      "  -0.13148319 -0.48126731]] [[-0.04201571 -1.03146311 -0.99324209  2.23482718  0.48731153  0.04141532\n",
      "  -1.87805052  0.23430846  0.89011077 -2.40878108  0.76822011 -0.80727184\n",
      "  -0.23139028  1.09034945  0.64625316 -0.73427032  1.71698982 -0.47911683\n",
      "  -1.04346074  0.44238988]\n",
      " [-0.54721813  1.44165435 -0.03969733 -2.75559713  0.49706977  0.86670278\n",
      "  -0.32585497  1.12179698 -1.17537943 -0.84064684 -0.50501519  1.17139473\n",
      "   0.37041284 -1.20415734  1.15911531  0.32632547  1.10950109  1.09967516\n",
      "   0.12099883 -1.07209097]\n",
      " [ 1.92242757  0.91221825  1.46721681  0.58711888 -1.08791583 -0.10710813\n",
      "  -2.23264201  0.49413365 -0.64166269  0.27184266 -0.74354951 -0.22969058\n",
      "  -1.42595282  1.14184497  0.32993744 -1.74405513  0.66325176  0.62165473\n",
      "   2.75657771  1.88827242]\n",
      " [ 0.78623627  1.36798176  0.63751017 -0.13390405 -0.26573913  2.27166243\n",
      "  -1.22809219  1.1004839  -0.42809814 -0.8377606  -0.95980261 -0.40412786\n",
      "  -0.22005714  1.99172938  1.19272635 -0.45822569  0.81218832 -0.74867033\n",
      "   0.79247226 -0.05058666]\n",
      " [-0.47727546 -2.38124478  0.96219759 -0.61945695  0.06275528  0.04984297\n",
      "   0.04124314 -1.44747979  0.70457429 -0.20346952 -0.56703614 -0.57907437\n",
      "   1.34388531  0.28976773 -0.26311703 -0.19356135  1.14737027  0.5274203\n",
      "  -0.13148319 -0.48126731]]\n"
     ]
    }
   ],
   "source": [
    "## tensor转为numpy数组\n",
    "numpy_array = pytorch_tensor1.numpy() #在cpu上可以直接转换\n",
    "numpy_array2 = pytorch_tensor1.cpu().numpy() #如果是在GPU上的tensor要先转化到CPU上\n",
    "print(numpy_array, numpy_array2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.1923, -0.9185,  1.0112,  0.7913,  1.0885, -0.7639,  0.2820, -0.0087,\n",
      "          0.5569,  0.7515, -1.1391,  0.0335, -0.4110,  0.9709, -1.1417,  0.7520,\n",
      "         -2.0836,  0.1689, -1.2848, -0.5820],\n",
      "        [-0.6657, -0.9849,  1.4465, -0.7792, -0.6676, -0.5991,  0.7583, -0.8425,\n",
      "          2.9495,  0.2893, -0.2704,  0.7059,  1.7492, -1.6006, -1.3384, -1.0034,\n",
      "         -1.3841,  1.6298,  0.3489,  0.6848],\n",
      "        [-1.3480, -1.1660,  0.4786, -1.8046,  0.5877,  0.9384,  0.9224,  0.2839,\n",
      "         -0.6237, -0.0499, -1.6318,  0.5280, -1.4194,  0.3315, -0.9152, -1.5580,\n",
      "         -0.9447, -0.6809, -0.9559,  0.1122],\n",
      "        [ 0.7240,  0.5458, -1.4345, -0.5413,  0.0420, -0.8908,  0.1923,  0.6055,\n",
      "         -1.8411,  0.6131, -0.2499,  0.7314, -2.2424,  0.5047,  0.2121,  0.6339,\n",
      "          0.7371,  1.5216,  1.2049, -0.4335],\n",
      "        [-1.2291,  1.3583,  0.9024,  1.7938,  0.1902,  0.9020, -0.5145,  1.4689,\n",
      "          0.4330, -2.0082, -0.8233,  1.9141, -0.7332, -0.0256,  0.0221, -1.4386,\n",
      "          0.3355,  1.6231, -1.1241, -0.4594],\n",
      "        [-0.2215,  1.8164,  1.7793, -0.5028, -0.8328, -1.1464, -1.7768,  0.6765,\n",
      "          0.7951, -0.6521, -0.9828, -0.4004,  1.6117, -0.3510,  0.4530,  0.6089,\n",
      "         -0.0649,  0.4721, -0.6940, -0.0065],\n",
      "        [-1.2996, -0.3227,  1.9808, -0.8683, -1.5142,  1.1525,  0.2090,  0.2194,\n",
      "         -0.1810,  1.4033, -1.0073, -0.7569, -0.5969,  0.6059, -0.1564,  0.1061,\n",
      "          1.7828, -0.0442, -1.0583, -0.9545],\n",
      "        [-0.6119, -0.1199, -1.3538, -1.3428, -0.2510, -1.1570, -0.1186,  1.6140,\n",
      "          1.9036,  0.8977,  1.2325, -0.3823,  0.5941, -0.4554,  1.0857, -0.1106,\n",
      "          1.4639,  1.6626,  0.0600, -0.9817],\n",
      "        [ 0.3391, -0.3266,  1.3907, -0.5398,  0.7442,  0.4902, -0.8547, -1.4843,\n",
      "          0.3950,  1.3950,  0.3886, -1.8161, -1.1877, -0.7468,  2.0854, -0.5749,\n",
      "          0.6333,  0.8705, -0.3347, -1.2983],\n",
      "        [ 1.4195, -0.8273,  0.0809,  0.0830,  0.3703,  0.4719,  1.1309, -0.4477,\n",
      "          1.5162, -2.1812, -0.5189, -0.3549, -0.6782,  1.2274,  1.3692, -1.7583,\n",
      "          0.9101, -0.4187, -0.7937, -1.0546]], device='cuda:0')\n",
      "tensor([[ 0.1923, -0.9185,  1.0112,  0.7913,  1.0885, -0.7639,  0.2820, -0.0087,\n",
      "          0.5569,  0.7515, -1.1391,  0.0335, -0.4110,  0.9709, -1.1417,  0.7520,\n",
      "         -2.0836,  0.1689, -1.2848, -0.5820],\n",
      "        [-0.6657, -0.9849,  1.4465, -0.7792, -0.6676, -0.5991,  0.7583, -0.8425,\n",
      "          2.9495,  0.2893, -0.2704,  0.7059,  1.7492, -1.6006, -1.3384, -1.0034,\n",
      "         -1.3841,  1.6298,  0.3489,  0.6848],\n",
      "        [-1.3480, -1.1660,  0.4786, -1.8046,  0.5877,  0.9384,  0.9224,  0.2839,\n",
      "         -0.6237, -0.0499, -1.6318,  0.5280, -1.4194,  0.3315, -0.9152, -1.5580,\n",
      "         -0.9447, -0.6809, -0.9559,  0.1122],\n",
      "        [ 0.7240,  0.5458, -1.4345, -0.5413,  0.0420, -0.8908,  0.1923,  0.6055,\n",
      "         -1.8411,  0.6131, -0.2499,  0.7314, -2.2424,  0.5047,  0.2121,  0.6339,\n",
      "          0.7371,  1.5216,  1.2049, -0.4335],\n",
      "        [-1.2291,  1.3583,  0.9024,  1.7938,  0.1902,  0.9020, -0.5145,  1.4689,\n",
      "          0.4330, -2.0082, -0.8233,  1.9141, -0.7332, -0.0256,  0.0221, -1.4386,\n",
      "          0.3355,  1.6231, -1.1241, -0.4594],\n",
      "        [-0.2215,  1.8164,  1.7793, -0.5028, -0.8328, -1.1464, -1.7768,  0.6765,\n",
      "          0.7951, -0.6521, -0.9828, -0.4004,  1.6117, -0.3510,  0.4530,  0.6089,\n",
      "         -0.0649,  0.4721, -0.6940, -0.0065],\n",
      "        [-1.2996, -0.3227,  1.9808, -0.8683, -1.5142,  1.1525,  0.2090,  0.2194,\n",
      "         -0.1810,  1.4033, -1.0073, -0.7569, -0.5969,  0.6059, -0.1564,  0.1061,\n",
      "          1.7828, -0.0442, -1.0583, -0.9545],\n",
      "        [-0.6119, -0.1199, -1.3538, -1.3428, -0.2510, -1.1570, -0.1186,  1.6140,\n",
      "          1.9036,  0.8977,  1.2325, -0.3823,  0.5941, -0.4554,  1.0857, -0.1106,\n",
      "          1.4639,  1.6626,  0.0600, -0.9817],\n",
      "        [ 0.3391, -0.3266,  1.3907, -0.5398,  0.7442,  0.4902, -0.8547, -1.4843,\n",
      "          0.3950,  1.3950,  0.3886, -1.8161, -1.1877, -0.7468,  2.0854, -0.5749,\n",
      "          0.6333,  0.8705, -0.3347, -1.2983],\n",
      "        [ 1.4195, -0.8273,  0.0809,  0.0830,  0.3703,  0.4719,  1.1309, -0.4477,\n",
      "          1.5162, -2.1812, -0.5189, -0.3549, -0.6782,  1.2274,  1.3692, -1.7583,\n",
      "          0.9101, -0.4187, -0.7937, -1.0546]])\n"
     ]
    }
   ],
   "source": [
    "## tensor转化导GPU上计算\n",
    "gpu_tensor = torch.randn(10,20).cuda(0)#将tensor放到第一个gpu上\n",
    "# gpu_tensor2 = torch.randn(10,20).cuda(1)#将tensor放到第二个gpu上,如果没有多块显卡报错\n",
    "print(gpu_tensor)\n",
    "cpu_tensor = gpu_tensor.cpu()\n",
    "print(cpu_tensor)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 20])\n",
      "torch.Size([5, 20])\n"
     ]
    }
   ],
   "source": [
    "# tensor的基本用法\n",
    "## 得到tensor的大小\n",
    "print(pytorch_tensor1.shape)#属性\n",
    "print(pytorch_tensor1.size())#方法"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "data": {
      "text/plain": "'torch.DoubleTensor'"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 得到tensor的数据类型\n",
    "pytorch_tensor1.type()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "data": {
      "text/plain": "2"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 得到tensor维度\n",
    "pytorch_tensor1.dim()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "data": {
      "text/plain": "100"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 得到tensor所有元素个数\n",
    "pytorch_tensor1.numel()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1.],\n",
      "        [1., 1.]])\n",
      "torch.FloatTensor\n"
     ]
    }
   ],
   "source": [
    "## 创建全为1的tensor\n",
    "x = torch.ones(2,2)\n",
    "print(x)\n",
    "print(x.type())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1.],\n",
      "        [1., 1.]])\n",
      "torch.FloatTensor\n",
      "tensor([[1, 1],\n",
      "        [1, 1]], dtype=torch.int32)\n",
      "torch.IntTensor\n"
     ]
    }
   ],
   "source": [
    "## 将tensor转化特定类型\n",
    "x = torch.ones(2,2)\n",
    "print(x)\n",
    "print(x.type())\n",
    "x = x.int()#这里可以写任意支持的类型，long、float等\n",
    "print(x)\n",
    "print(x.type())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.8918,  1.9159,  0.0298],\n",
      "        [ 0.0556, -0.2020,  1.0884],\n",
      "        [ 1.8685, -0.1276,  1.5608],\n",
      "        [ 0.0374, -1.1993, -0.8290]])\n"
     ]
    }
   ],
   "source": [
    "## 生成随机数据\n",
    "x = torch.randn(4, 3)\n",
    "print(x)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.9159, 1.0884, 1.8685, 0.0374])\n",
      "tensor([1, 2, 0, 0])\n"
     ]
    }
   ],
   "source": [
    "## 寻找矩阵中每行中最大值及其索引\n",
    "max_value, max_idx = torch.max(x , dim=1)\n",
    "print(max_value)\n",
    "print(max_idx)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 2])\n",
      "torch.Size([1, 2, 2])\n",
      "torch.Size([1, 2, 2, 1])\n",
      "torch.Size([2, 2, 1])\n",
      "torch.Size([2, 2])\n"
     ]
    }
   ],
   "source": [
    "## 增加/减少tensor的维度\n",
    "x = torch.ones(2, 2)\n",
    "print(x.shape)\n",
    "x = x.unsqueeze(0)#增加第一维度\n",
    "print(x.shape)\n",
    "x = x.unsqueeze(-1)#最后增加一维度\n",
    "print(x.shape)\n",
    "x = x.squeeze(0)#减少一维度\n",
    "print(x.shape)\n",
    "x = x.squeeze()#去掉所有一维的维度\n",
    "print(x.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 4, 5])\n",
      "torch.Size([4, 3, 5])\n",
      "torch.Size([5, 3, 4])\n"
     ]
    }
   ],
   "source": [
    "## tensor的维度排列\n",
    "x = torch.randn(3, 4, 5)\n",
    "print(x.shape)\n",
    "x = x.permute(1, 0, 2)#重新排列维度\n",
    "print(x.shape)\n",
    "x = x.transpose(0, 2)#交换维度\n",
    "print(x.shape)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 4, 5])\n",
      "torch.Size([30, 2])\n",
      "torch.Size([3, 20])\n"
     ]
    }
   ],
   "source": [
    "## tensor的维度转化\n",
    "x = torch.randn(3, 4, 5)\n",
    "print(x.shape)\n",
    "x = x.view(-1, 2)#-1表示根据其他维度自行确认该维度\n",
    "print(x.shape)\n",
    "x = x.view(3, 20)#转化为指定维度\n",
    "print(x.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 4, 5])\n",
      "torch.Size([1, 3, 4, 5])\n"
     ]
    }
   ],
   "source": [
    "## torch的inplace操作（在操作符后加下划线“_”）\n",
    "x = torch.randn(3,4,5)\n",
    "print(x.shape)\n",
    "x.unsqueeze_(0)\n",
    "print(x.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Variable的基本用法与自动求导\n",
    "'''\n",
    "（1）Variable.data ：data属性即是本身的Tensor；\n",
    "（2）Variable.grad：通过反向传播后，grad属性即是梯度指；\n",
    "（3）Variable.grad_fn：构建过程及方式即Variable的grad_fn属性\n",
    "'''"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [],
   "source": [
    "from torch.autograd import Variable as var"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None tensor([[-1.6671e-01, -1.8077e+00,  7.8527e-03, -4.3268e-01, -2.1746e-02],\n",
      "        [-5.4532e-01, -1.2035e+00,  1.9933e+00,  8.6060e-01,  1.7089e+00],\n",
      "        [-2.6111e+00, -1.8163e+00,  4.2904e-01, -1.5935e+00,  8.2917e-01],\n",
      "        [ 1.3152e+00,  2.1132e+00, -1.4593e+00, -2.5849e-01, -8.4379e-01],\n",
      "        [ 5.5118e-01, -1.0175e-01, -1.7066e+00,  1.5090e+00,  5.6854e-01],\n",
      "        [-1.0335e+00, -1.4276e-01,  4.5841e-01,  2.1120e-01, -2.3750e-02],\n",
      "        [ 2.9842e-01, -1.9570e+00, -1.1439e-01,  3.1577e-01, -9.7095e-01],\n",
      "        [-6.4953e-01, -1.8488e+00,  1.0312e+00, -2.4716e-01,  1.0454e-01],\n",
      "        [-5.4660e-01, -1.1391e+00, -1.7222e-03, -1.3096e+00,  2.9963e-01],\n",
      "        [ 5.1791e-01,  4.1057e-01, -6.1578e-01, -5.6396e-01, -5.2962e-01]],\n",
      "       requires_grad=True)\n",
      "None tensor([[ 0.5282,  0.6060, -0.7697,  1.2066, -1.5311],\n",
      "        [-1.6528,  0.7665, -0.6950,  1.6728, -1.6597],\n",
      "        [ 0.8511, -0.0610, -0.7873,  1.8583, -0.7553],\n",
      "        [ 0.9959, -0.5003, -1.3746,  1.2092, -0.6258],\n",
      "        [-0.7058, -0.9746,  0.2443,  1.1383,  1.7418],\n",
      "        [-1.8472, -0.0667,  1.4959, -0.0604,  0.0733],\n",
      "        [-0.0791,  0.0687, -0.3561,  0.2423, -0.2205],\n",
      "        [ 0.8294,  0.8267,  0.5207,  0.7053,  0.0885],\n",
      "        [ 0.5202,  1.8613, -0.5420, -0.9630,  0.2083],\n",
      "        [-0.3778,  1.1202,  0.7607, -0.9215, -1.0011]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "## tnsor转化为variable\n",
    "x_tensor = torch.randn(10, 5)\n",
    "y_tensor = torch.randn(10, 5)\n",
    "x = var(x_tensor, requires_grad = True) #requires_grad求梯度参数，true表示求\n",
    "print(x.grad, x)\n",
    "y = var(y_tensor, requires_grad = True)\n",
    "print(y.grad, y)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-7.1171)\n",
      "<SumBackward0 object at 0x00000272CC41FF08>\n"
     ]
    }
   ],
   "source": [
    "## variable的运算\n",
    "z = torch.sum(x + y)\n",
    "print(z.data) #data属性即是本身的Tensor；\n",
    "print(z.grad_fn) #构建过程及方式即Variable的grad_fn属性"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.]])\n",
      "tensor([[1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "## 求z关于梯度x和y的梯度值\n",
    "z.backward()\n",
    "print(x.grad) #前面刚建立x，y是没grad的，backward之后才有grad，因为backward才链式求导了\n",
    "print(y.grad)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2.], requires_grad=True)\n",
      "tensor([19.], grad_fn=<AddBackward0>)\n",
      "tensor([8.])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "## torch的自动求导\n",
    "'''定义一个简单函数并初始化'''\n",
    "x = var(torch.Tensor([2]), requires_grad = True)#注意这了是大写T，小写t就成了转化\n",
    "y = x + 2\n",
    "z = y ** 2 + 3\n",
    "print(x)\n",
    "print(z)\n",
    "z.backward()\n",
    "print(x.grad)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.3869, -1.8441,  0.1833, -0.1408, -0.2587, -0.7610, -2.1376,  0.6763,\n",
      "         -1.0739, -0.6241, -0.0241, -2.1187,  0.4243,  0.0054,  0.9501,  0.2229,\n",
      "         -0.0790,  0.8490, -0.1119,  0.2383],\n",
      "        [-0.1852, -0.3263, -0.6571,  1.0523,  0.9925,  0.2280, -0.1788, -0.9651,\n",
      "         -0.6667, -0.2314, -0.0356, -0.5279,  0.7396,  0.7772, -1.3920, -2.9456,\n",
      "          1.4288,  0.5675, -0.0079, -0.0601],\n",
      "        [ 1.7998, -1.2727,  0.2236,  0.5868,  1.3777, -0.2302, -0.6145,  1.2788,\n",
      "          0.0730,  0.7921,  0.6467, -1.0261, -0.9977, -0.5089,  1.0232, -0.9956,\n",
      "          0.3353, -0.9120,  0.1259, -1.5256],\n",
      "        [-0.9210, -0.1081, -0.6501,  0.3461,  1.3688, -0.7816, -0.8096,  0.4614,\n",
      "         -0.9482,  1.2167, -0.8059, -0.8693,  1.4616,  2.6624,  0.3813,  1.0205,\n",
      "         -0.4351,  1.0234,  0.4509,  0.2319],\n",
      "        [ 1.4200, -0.6489, -0.0927, -0.5744,  1.5813,  0.4184,  0.3568, -0.0103,\n",
      "          1.0604,  0.0713,  0.1955,  1.1796, -0.0345, -1.7936, -1.5906, -0.1566,\n",
      "          0.0846, -0.3056, -0.9126, -0.1265],\n",
      "        [-1.0950, -0.0345, -1.6918, -0.8541,  0.9235, -0.5466,  1.7160,  1.8106,\n",
      "         -0.4205,  0.2582,  1.5001,  0.8405, -0.3357,  2.2590,  0.6062,  1.4031,\n",
      "          0.4877,  1.6885, -0.9445, -0.0105],\n",
      "        [-0.8259, -1.0641, -0.7721,  0.2107, -0.7321,  1.5664, -0.8521,  0.3133,\n",
      "          0.4926, -0.1529, -0.1219, -0.0711,  0.9608,  1.2962, -1.0278, -0.2696,\n",
      "          0.8392, -1.6236, -2.1163,  0.6692],\n",
      "        [ 0.8629,  1.2017,  0.3579,  2.9376,  0.0108,  0.8427,  2.7556, -2.0998,\n",
      "          1.4434, -0.6213,  0.7501, -0.6663, -1.7753,  0.6539,  0.3339, -2.6795,\n",
      "          0.1112, -1.4386,  0.0678,  0.3002],\n",
      "        [ 1.2358, -0.7317, -0.4436, -1.2399, -0.0107, -0.4605,  0.3126,  0.7854,\n",
      "         -0.4436,  0.8039,  0.8830, -0.3251,  0.6270,  0.0877,  0.3140, -1.7938,\n",
      "         -0.1223,  1.0599, -0.5627, -0.1601],\n",
      "        [ 0.9796, -0.3502,  0.8667,  0.6990,  0.3893,  0.8322, -0.9571, -0.6080,\n",
      "          1.5556, -2.5277, -2.3824, -1.3544,  0.7479,  0.6205,  0.5498,  1.5148,\n",
      "         -0.4099,  0.1127, -0.4078, -0.9416]], requires_grad=True)\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "tensor([[-0.0727,  0.0264, -0.0048, -0.0980,  0.0011, -0.0180, -0.0112,  0.0400,\n",
      "          0.0011, -0.0602,  0.0912, -0.0541,  0.0771,  0.0952, -0.0640, -0.0479,\n",
      "         -0.0303, -0.0258,  0.0025, -0.1333],\n",
      "        [-0.0727,  0.0264, -0.0048, -0.0980,  0.0011, -0.0180, -0.0112,  0.0400,\n",
      "          0.0011, -0.0602,  0.0912, -0.0541,  0.0771,  0.0952, -0.0640, -0.0479,\n",
      "         -0.0303, -0.0258,  0.0025, -0.1333],\n",
      "        [-0.0727,  0.0264, -0.0048, -0.0980,  0.0011, -0.0180, -0.0112,  0.0400,\n",
      "          0.0011, -0.0602,  0.0912, -0.0541,  0.0771,  0.0952, -0.0640, -0.0479,\n",
      "         -0.0303, -0.0258,  0.0025, -0.1333],\n",
      "        [-0.0727,  0.0264, -0.0048, -0.0980,  0.0011, -0.0180, -0.0112,  0.0400,\n",
      "          0.0011, -0.0602,  0.0912, -0.0541,  0.0771,  0.0952, -0.0640, -0.0479,\n",
      "         -0.0303, -0.0258,  0.0025, -0.1333],\n",
      "        [-0.0727,  0.0264, -0.0048, -0.0980,  0.0011, -0.0180, -0.0112,  0.0400,\n",
      "          0.0011, -0.0602,  0.0912, -0.0541,  0.0771,  0.0952, -0.0640, -0.0479,\n",
      "         -0.0303, -0.0258,  0.0025, -0.1333],\n",
      "        [-0.0727,  0.0264, -0.0048, -0.0980,  0.0011, -0.0180, -0.0112,  0.0400,\n",
      "          0.0011, -0.0602,  0.0912, -0.0541,  0.0771,  0.0952, -0.0640, -0.0479,\n",
      "         -0.0303, -0.0258,  0.0025, -0.1333],\n",
      "        [-0.0727,  0.0264, -0.0048, -0.0980,  0.0011, -0.0180, -0.0112,  0.0400,\n",
      "          0.0011, -0.0602,  0.0912, -0.0541,  0.0771,  0.0952, -0.0640, -0.0479,\n",
      "         -0.0303, -0.0258,  0.0025, -0.1333],\n",
      "        [-0.0727,  0.0264, -0.0048, -0.0980,  0.0011, -0.0180, -0.0112,  0.0400,\n",
      "          0.0011, -0.0602,  0.0912, -0.0541,  0.0771,  0.0952, -0.0640, -0.0479,\n",
      "         -0.0303, -0.0258,  0.0025, -0.1333],\n",
      "        [-0.0727,  0.0264, -0.0048, -0.0980,  0.0011, -0.0180, -0.0112,  0.0400,\n",
      "          0.0011, -0.0602,  0.0912, -0.0541,  0.0771,  0.0952, -0.0640, -0.0479,\n",
      "         -0.0303, -0.0258,  0.0025, -0.1333],\n",
      "        [-0.0727,  0.0264, -0.0048, -0.0980,  0.0011, -0.0180, -0.0112,  0.0400,\n",
      "          0.0011, -0.0602,  0.0912, -0.0541,  0.0771,  0.0952, -0.0640, -0.0479,\n",
      "         -0.0303, -0.0258,  0.0025, -0.1333]])\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "tensor(0.6338, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "'''定义一个复杂函数并初始化'''\n",
    "x = var(torch.randn(10, 20), requires_grad = True)#10rows,20columns\n",
    "y = var(torch.randn(10, 5), requires_grad = True)#10rows,5columns,reflect to rows of x\n",
    "w = var(torch.randn(20, 5), requires_grad = True)#20rows,xcolumns,reflect to columns of x\n",
    "out = torch.mean(y - torch.matmul(x, w))\n",
    "out.backward()\n",
    "print(x)\n",
    "print('~'*50)\n",
    "print(x.grad)\n",
    "print('~'*50)\n",
    "print(out)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1.],\n",
      "        [1., 1.],\n",
      "        [1., 1.],\n",
      "        [1., 1.],\n",
      "        [1., 1.],\n",
      "        [1., 1.],\n",
      "        [1., 1.],\n",
      "        [1., 1.],\n",
      "        [1., 1.],\n",
      "        [1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "# 数据预处理\n",
    "## 读取数据\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}